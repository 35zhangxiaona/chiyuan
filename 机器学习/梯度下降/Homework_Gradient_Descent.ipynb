{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 12 题 ######################################\n",
    "#    Implement Batch Gradient Descent with early stopping for Softmax Regression(without using SKLEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 思路：准备数据——转换one-hot——求训练集loss和梯度——求验证集loss，判断验证集loss是否不再下降——返回验证集最小loss，停止\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target\n",
    "N = len(y)\n",
    "X_bias = np.c_[np.ones((N,1)), X]\n",
    "test_ratio, val_ratio = 0.2, 0.2\n",
    "np.random.seed(42)\n",
    "shuffle_indeces = np.random.permutation(N)\n",
    "len_val = np.int(N * val_ratio)\n",
    "len_test = np.int(N * test_ratio)\n",
    "X_test = X_bias[shuffle_indeces[:len_test]]\n",
    "y_test = y[shuffle_indeces[:len_test]]\n",
    "X_val = X_bias[shuffle_indeces[len_test:len_val+len_test]]\n",
    "y_val = y[shuffle_indeces[len_test:len_val+len_test]]\n",
    "X_train = X_bias[shuffle_indeces[len_val+len_test:]]\n",
    "y_train = y[shuffle_indeces[len_val+len_test:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    num_classes = len(np.unique(y)) #这样写其实不严格，比如数据较少时，len(np.unique(np.array([1,2,3,4,9]))) = 5\n",
    "    sparse_y = np.zeros((len(y), num_classes))\n",
    "    sparse_y[np.arange(len(y)),y] = 1   \n",
    "    return sparse_y\n",
    "y_train = to_one_hot(y_train)\n",
    "y_val = to_one_hot(y_val)\n",
    "y_test = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 1]\n",
      " [3 2 0]\n",
      " [3 0 0]]\n",
      "[[0.         0.66666667 0.33333333]\n",
      " [0.6        0.4        0.        ]\n",
      " [1.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.4       , 0.33333333],\n",
       "       [1.        , 0.4       , 0.        ],\n",
       "       [1.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randint(5, size=[3,3])\n",
    "print(X)\n",
    "print(X/np.sum(X,axis = 1, keepdims=True))\n",
    "X/np.sum(X,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = np.size(X_bias,1)\n",
    "n_output = y.max()+1\n",
    "theta = np.random.randn(n_input, n_output)\n",
    "def softmax(scores):\n",
    "    exp_scores = np.exp(scores)\n",
    "    return exp_scores/np.sum(exp_scores,axis=1, keepdims=True)\n",
    "#     return exp_scores/np.sum(exp_scores,axis=1) #错误写法：这样sum的结果不是一个列向量，而只是一个向量向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 ,validation_loss: 4.526865653040438)\n",
      "iter: 500 ,validation_loss: 0.6069095202781684)\n",
      "iter: 1000 ,validation_loss: 0.5834884313558759)\n",
      "1307 0.5817246339823058\n",
      "1308 0.5817246342698289 early stopping!\n"
     ]
    }
   ],
   "source": [
    "# 注意： 这里加了early stopping之后，准确率无法达到百分之百，不加时可以达到。\n",
    "# 当early stop 条件满足时，实际上取得不是最好的theta而是最好的theta之后一轮迭代的theta\n",
    "## np.zeros, np.ones 加双括号！！\n",
    "eta = 0.07\n",
    "weight_decay = 0.1\n",
    "EPSILON = 1e-10\n",
    "best_loss = np.inf\n",
    "EARLY_STOPPING = True\n",
    "m = len(y_train)\n",
    "for iter in range(5001):\n",
    "    scores = np.dot(X_train, theta)\n",
    "    preds = softmax(scores)\n",
    "    error = preds - y_train\n",
    "#     l2_loss = np.sum(np.square(theta))\n",
    "    l2_reg = np.sum(np.square(theta[1:])) #正则化时不对w0进行惩罚,而theta的第一行对应每个二分类器的w0（对应X矩阵的第一列bias项）\n",
    "    train_loss = -np.mean(np.sum(y_train * np.log(preds + EPSILON), axis = 1)) + weight_decay * l2_reg\n",
    "    gradients = 1/m * X_train.T.dot(error) + weight_decay * np.r_[ np.zeros((1,n_output)), theta[1:]] #将λ/m合并在了λ里面？？？\n",
    "    # 注意：由于l2_reg惩罚项不含w0，所以这里梯度的reg也要对去掉w0之外的w进行，但为了保持维度一致，可以在去除第一行之后的theta\n",
    "    # 的第一行补上零\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "    preds_val = softmax(X_val.dot(theta))\n",
    "    val_loss = -np.mean(np.sum(y_val * np.log(preds_val), axis = 1)) + weight_decay * l2_reg\n",
    "    if iter %500 == 0:\n",
    "        print('iter: {0} ,validation_loss: {1})'.format(iter, val_loss))\n",
    "    if EARLY_STOPPING:\n",
    "        if best_loss>val_loss:\n",
    "            best_loss = val_loss\n",
    "        else:\n",
    "            print(iter-1, best_loss)\n",
    "            print(iter, val_loss, 'early stopping!')\n",
    "            break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "predict = np.argmax(softmax(X_val.dot(theta)),axis=1)\n",
    "y_val_label  = np.argmax(y_val, axis=1)\n",
    "accuracy = np.mean((predict==y_val_label))\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 2, 0, 0, 1, 2, 2, 1, 2, 1, 2,\n",
       "       1, 0, 2, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
